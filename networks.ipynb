{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 NumPy 中，多维数组（即 ndarray）的维度顺序通常是按从外到内的顺序排列的。例如，对于一个二维数组（矩阵），第一个维度代表行，第二个维度代表列。对于一个三维数组，维度顺序是深度、高度和宽度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 导入相应的包\n",
    "import numpy as np\n",
    "tmp = np.zeros((4,))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络类型\n",
    "1. FC 全连接层\n",
    "2. CNN 卷积层\n",
    "3. 池化层\n",
    "3. LayerNorm 层\n",
    "4. BatchNorm 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层\n",
    "\"\"\"\n",
    "前向传播:\n",
    "    - z = xW^T + b\n",
    "\n",
    "反向传播: dz为batch_size * output_dim\n",
    "    - dz/dW = dz^T * X, output_dim * input_dim\n",
    "    - dz/db = sum(dz), output_dim\n",
    "    - dz/dX = dz * W, batch_size * input_dim (对应上一层的输出)\n",
    "\n",
    "复杂度分析:\n",
    "    - 前向传播: O(batch_size, input_dim, output_dim)\n",
    "    - 后向传播: O(batch_size, input_dim, output_dim)\n",
    "\"\"\"\n",
    "class FullConnectedLayer:\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=1e-2):\n",
    "        \n",
    "        # 初始化参数，randn是正太分布初始化\n",
    "        self.W = np.random.randn(output_dim, input_dim)\n",
    "        self.b = np.random.randn(output_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # 全连接层计算 z = Wx + b\n",
    "        # x: batch_size * input_dim \n",
    "        # 维度不同时，numpy会通过广播机制进行补齐\n",
    "        self.input = x\n",
    "        self.z = np.dot(x, self.W.T) + self.b \n",
    "\n",
    "        # 注意要保留计算结果，用于反向传播\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, dz):\n",
    "\n",
    "        # 计算梯度\n",
    "        # dz : batch * output_dim\n",
    "        dW = np.dot(dz.T, self.input)\n",
    "        db = np.sum(dz, axis=0)\n",
    "        dX = np.dot(dz, self.W)\n",
    "\n",
    "        # 更新参数\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "\n",
    "        return dX        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 卷积层\n",
    "class ConvLayer:\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, padding=0):\n",
    "        \n",
    "        # 保留参数\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding # 当padding = (kernel_size - 1)/2 时, 卷积前后大小不变\n",
    "\n",
    "        # 初始化权重\n",
    "        self.W = np.random.randn(output_channels, input_channels, kernel_size, kernel_size)\n",
    "        self.b = np.random.randn(output_channels)  # 输出维度代表卷积核的数量，每个卷积核共用一个bias\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        # X: batch_size * input_channels * height * width\n",
    "        N, C_in, H_in, W_in = X.shape\n",
    "        C_out, C_in, K_h, K_w = self.W.shape\n",
    "\n",
    "        # 计算输出维度\n",
    "        H_out = (H_in - K_h + 2 * self.padding) // self.stride + 1\n",
    "        W_out = (W_in - K_w + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        # 初始化输出\n",
    "        Z = np.zeros(N, C_out, H_out, W_out)\n",
    "\n",
    "        # padding\n",
    "        X_padded = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "\n",
    "        # 卷积计算\n",
    "        for n in range(N):\n",
    "            for c_out in range(C_out):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + K_h\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + K_w\n",
    "                        # 计算输出                \n",
    "                        Z[n, c_out, i, j] = np.sum(X_padded[n, :, h_start:h_end, w_start:w_end] * self.W[c_out, :, :, :]) + self.b[c_out]\n",
    "\n",
    "        self.X = X\n",
    "        self.Z = Z\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        # dZ 的形状: (N, C_out, H_out, W_out)\n",
    "        N, C_out, H_out, W_out = dZ.shape\n",
    "        C_out, C_in, K_h, K_w = self.W.shape\n",
    "        N, C_in, H_in, W_in = self.X.shape\n",
    "\n",
    "        # 填充输入\n",
    "        X_padded = np.pad(self.X, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "\n",
    "        # 初始化梯度\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dX = np.zeros_like(X_padded)\n",
    "\n",
    "        # 计算 W 和 b 的梯度\n",
    "        for n in range(N):\n",
    "            for c_out in range(C_out):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * self.stride\n",
    "                        h_end = h_start + K_h\n",
    "                        w_start = j * self.stride\n",
    "                        w_end = w_start + K_w\n",
    "\n",
    "                        # 对 W 和 b 的梯度累加\n",
    "                        dW[c_out, :, :, :] += dZ[n, c_out, i, j] * X_padded[n, :, h_start:h_end, w_start:w_end]\n",
    "                        db[c_out] += dZ[n, c_out, i, j]\n",
    "                        \n",
    "                        # 反向传播输入梯度\n",
    "                        dX[n, :, h_start:h_end, w_start:w_end] += dZ[n, c_out, i, j] * self.W[c_out, :, :, :]\n",
    "\n",
    "        # 去掉填充\n",
    "        if self.padding > 0:\n",
    "            dX = dX[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.W -= self.learning_rate * dW\n",
    "        self.b -= self.learning_rate * db\n",
    "\n",
    "        return dX    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 对可学习参数 $ \\gamma $ 和 $ \\beta $ 的梯度：\n",
    "\n",
    "- 对 $ \\gamma $ 的梯度：\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\gamma_i} = \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial y_{n,i}} \\cdot \\hat{x}_{n,i}\n",
    "  $$\n",
    "\n",
    "- 对 $ \\beta $ 的梯度：\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial \\beta_i} = \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial y_{n,i}}\n",
    "  $$\n",
    "\n",
    "### 2. 对均值 $ \\mu $ 的梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\sum_{i=1}^{d} \\left( \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} \\right) + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{-2}{d} \\sum_{i=1}^{d} (x_i - \\mu)\n",
    "$$\n",
    "\n",
    "### 3. 对方差 $ \\sigma^2 $ 的梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\sigma^2} = \\sum_{i=1}^{d} \\left( \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot \\frac{-(x_i - \\mu)}{2(\\sigma^2 + \\epsilon)^{3/2}} \\right)\n",
    "$$\n",
    "\n",
    "### 4. 最终对 $ x_i $ 的梯度：\n",
    "\n",
    "对 $ x_i $ 的总梯度由三部分组成：\n",
    "1. $ x_i \\to \\hat{x}_i \\to L $\n",
    "2. $ x_i \\to \\mu \\to \\hat{x}_i \\to L$\n",
    "3. $ x_i \\to \\sigma^2 \\to \\hat{x}_i \\to L $\n",
    "\n",
    "总的梯度为：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{1}{d} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{2(x_i - \\mu)}{d}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm\n",
    "\"\"\"\n",
    "- 时间复杂度分析\n",
    "    - 前向传播: O(ND)\n",
    "    - 后向传播: O(ND)\n",
    "\"\"\"\n",
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, input_dim, epsilon=1e-5):\n",
    "\n",
    "        self.gamma = np.ones(input_dim)\n",
    "        self.beta = np.zeros(input_dim)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        # 求均值、方差\n",
    "        # X : batch_size * input_dim\n",
    "        mu = np.mean(X, axis=-1, keepdims=True)\n",
    "        var = np.var(X, axis=-1, keepdims=True)\n",
    "\n",
    "        # 标准化\n",
    "        X_hat = (X - mu) / np.sqrt(var + self.epsilon)\n",
    "\n",
    "        # 放缩\n",
    "        Y = self.gamma * X_hat + self.beta\n",
    "        self.X = X\n",
    "        self.mu = mu\n",
    "        self.var = var\n",
    "        self.X_hat = X_hat\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "\n",
    "        # 反向传播\n",
    "        # dY : batch_size, output_dim\n",
    "        N, D = dY.shape\n",
    "\n",
    "        # 求本层梯度\n",
    "        d_gamma = np.sum(self.X_hat * dY, axis=0)\n",
    "        d_beta = np.sum(dY, axis=0)\n",
    "\n",
    "        # 求反向梯度\n",
    "        d_X_hat = self.gamma * dY\n",
    "        d_var = np.sum(d_X_hat * (self.X - self.mu) * -0.5 * (self.var + self.epsilon) ** -1.5, axis=-1, keepdims=True)\n",
    "        d_mu = np.sum(d_X_hat * -1 / np.sqrt(self.var + self.epsilon), axis=-1, keepdims=True) + d_var * np.mean(-2 * (self.X - self.mu), axis=-1, keepdims=True)    \n",
    "        dx = d_X_hat / np.sqrt(self.var + self.epsilon) + d_var * 2 * (self.X - self.mu) / D + d_mu / D\n",
    "\n",
    "        # 更新参数\n",
    "        self.gamma -= self.lr * d_gamma\n",
    "        self.beta -= self.lr * d_beta\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm\n",
    "\"\"\"\n",
    "- 时间复杂度分析\n",
    "    - 前向传播: O(ND)\n",
    "    - 后向传播: O(ND)\n",
    "\"\"\"\n",
    "class BatchNorm:\n",
    "\n",
    "    def __init__(self, input_dim, epsilon=1e-5):\n",
    "\n",
    "        self.gamma = np.ones(input_dim)\n",
    "        self.beta = np.zeros(input_dim)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, X, isTrain=True):\n",
    "\n",
    "        if isTrain:\n",
    "            # 计算均值和方差\n",
    "            mu = np.mean(X, axis=0, keepdims=True)\n",
    "            var = np.var(X, axis=0, keepdims=True)\n",
    "\n",
    "            # 归一化\n",
    "            X_hat = (X - mu) / np.sqrt(var + self.epsilon)\n",
    "\n",
    "            # 计算输出\n",
    "            y = self.gamma * X_hat + self.beta\n",
    "\n",
    "            # 保存用于反向传播\n",
    "            self.mu = mu\n",
    "            self.var = var\n",
    "            self.X_hat = X_hat\n",
    "            self.X = X\n",
    "        else:\n",
    "            X_hat = (X - self.mu) / np.sqrt(self.var + self.epsilon)\n",
    "        \n",
    "        y = self.gamma * X_hat + self.beta\n",
    "        return y\n",
    "\n",
    "    def backward(self, dY):\n",
    "\n",
    "        N, D = self.X.shape\n",
    "\n",
    "        # 计算参数的梯度\n",
    "        d_gamma = np.sum(dY * self.X_hat, axis=0)\n",
    "        d_beta = np.sum(dY, axis=0)\n",
    "\n",
    "        # 计算反向传播的梯度\n",
    "        d_X_hat = dY * self.gamma\n",
    "        d_var = np.sum(d_X_hat * -0.5 * (self.X - self.mu) * (self.var + self.epsilon) ** -1/5, axis=0, keepdims=True)\n",
    "        d_mu = np.sum(d_X_hat * -1 / np.sqrt(self.var + self.epsilon), axis=0, keepdims=True) + d_var * np.mean(-2 * (self.X - self.mu), axis=0, keepdims=True)\n",
    "        dX = d_X_hat / np.sqrt(self.var + self.epsilon) + d_var * 2/N * (self.X - self.mu) + d_mu / N\n",
    "\n",
    "        return dX\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(3, 5)\n",
      "(3, 5)\n",
      "[[ 0.54846513  0.96159119 -0.2132476   0.58320184  1.02084592]\n",
      " [-0.02489036 -0.93834391  0.31020801 -0.44293322  0.58189601]\n",
      " [ 1.79345817  0.41517114 -1.05214362 -0.47283479 -2.81505693]]\n",
      "[[ 0.54846513  0.96159119 -0.2132476   0.58320184  1.02084592]\n",
      " [-0.02489036 -0.93834391  0.31020801 -0.44293322  0.58189601]\n",
      " [ 1.79345817  0.41517114 -1.05214362 -0.47283479 -2.81505693]]\n"
     ]
    }
   ],
   "source": [
    "gamma = np.ones(5)\n",
    "beta = np.ones(5)\n",
    "X = np.random.randn(3, 5)\n",
    "print(gamma.shape)\n",
    "print(X.shape)\n",
    "Y = gamma * X\n",
    "print(Y.shape)\n",
    "print(X)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数\n",
    "1. ReLU 激活函数\n",
    "2. Sigmoid 激活函数\n",
    "3. Softmax 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 激活函数 \n",
    "\"\"\"\n",
    "前向传播\n",
    "    - output = max(X, 0)\n",
    "\n",
    "反向传播\n",
    "    - dz/dX = dz * (X > 0)\n",
    "\n",
    "复杂度分析\n",
    "    - 前向传播: O(batch_size, input_dim)\n",
    "    - 后向传播: O(batch_size, input_dim)\n",
    "\"\"\"\n",
    "class ReLULayer:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.x = x\n",
    "        self.output = np.maximum(self.x, 0)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "\n",
    "        dz = doutput * (self.input > 0).astype(float)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数\n",
    "1. MSE 损失函数\n",
    "2. MAE 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE 损失函数\n",
    "\"\"\"\n",
    "前向传播:\n",
    "    - output = 1/N * sum(y_pred - y_true)\n",
    "\n",
    "反向传播:\n",
    "    - dL/dX = 2/batch_size * (y_pred - y_true)\n",
    "\n",
    "复杂度分析:\n",
    "    - 前向传播: O(batch_size, input_dim)\n",
    "    - 后向传播: O(batch_size, input_dim)\n",
    "\"\"\"\n",
    "class MSELoss:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # 计算loss\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "\n",
    "        loss = np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "\n",
    "        # 计算梯度\n",
    "        # dL: batch_size * output*dim\n",
    "        batch_size = self.y_pred.shape[0]\n",
    "        dL = (2 * (self.y_pred - self.y_true)) / batch_size\n",
    "\n",
    "        return dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  9 11  8 10]\n",
      "[15 15 15]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'aaaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 15\u001b[0m \u001b[43maaaa\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aaaa' is not defined"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "fc_layer = FullConnectedLayer(input_dim=5, output_dim=1)\n",
    "loss_function = MSELoss()\n",
    "\n",
    "# 输入和目标\n",
    "X = np.array([[1, 2, 3, 4, 5], \n",
    "              [5, 4, 3, 2, 1], \n",
    "              [1, 3, 5, 2, 4]])\n",
    "y = np.array([[1], \n",
    "              [0], \n",
    "              [1]])\n",
    "\n",
    "print(np.sum(X, axis=0))\n",
    "print(np.sum(X, axis=1))\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# 前向传播\n",
    "z = fc_layer.forward(X)\n",
    "loss = loss_function.forward(z, y)\n",
    "\n",
    "# 反向传播\n",
    "dz = loss_function.backward()\n",
    "dx = fc_layer.backward(dz)\n",
    "\n",
    "# print(dz.shape)\n",
    "# print(dz)\n",
    "\n",
    "# print(\"Loss:\", loss)\n",
    "# print(\"Gradient wrt Input:\", dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
